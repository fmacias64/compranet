[core]
#logging_conf_file=./logging.cfg
default-scheduler-host=localhost
default-scheduler-port=8082


[DEFAULT]
#root_path: ../data
#raw_path: raw
#data_path: data
#etl_path: etl
#tmp_path: tmp
#metadata_path: metadata
#group: research
#application: dummy-app
#client: dpa-template


[Ingestpipeline]
bash_pipelines: unidades_compradoras, claves_salariales
docker_pipelines: declaranet
# ingest pipelines: unidades_compradoras, funcionarios, compranet


[bash_ingestion_s3]
bash_scripts = ./ingest/bash_scripts/
local_path = ../data/
raw_bucket = s3://dpa-compranet/etl/


[Runpipelines]
raw_schema: raw
clean_schema: clean
temp: ../data/temp

[Crawlpipelines]
# Declaranet depende de funcionarios
# crawl_pipeline: declaranet

[localToS3]
s3bucket = dpa-compranet


[RTask]

[PythonTask]

[PySparkTask]

[SqoopTask]

[HadoopTask]

[RawData]
