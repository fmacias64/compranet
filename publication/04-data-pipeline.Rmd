# Data Pipeline

Our pipeline is entirely orchestrated through Spotify's Luigi. We have 5 different original sources of data:

- The current distribution of government officials in the Public Federal Administration (APF)
- All public procurement contracts available in Compranet
- All buying units listed in Compranet
- Shell companies identified by the Tax department
- Salary codes for officials in the APF

All 5 sources are downloaded in parallel when the pipeline is triggered. Once we have all clean names for public officials, we construct our last data source, by scraping the interest declaration for each one of those names, and parsing the work experience section.

```{r pipeline, eval = F}
knitr::include_graphics("images/pipeline.png")
```

Para este proyecto dockerizamos un ambiente luigi-worker que inicializa el proyecto cargando los módulos necesarios para correr, empaquetando el proyecto en un paquete de python, creando las imágenes de docker y corriendo un docker-compose para exponer los puertos.

El Pipeline compienza en política_preventiva.py que tiene tres clases principales 1)Ingest, 2)ETL y 3)Model. Toma la configuración de los pipelene_Tasks a ingestar del luigi.cfg y comienza un proceso estandarizado de ingesta. [pipeline/ingest/ingest_orchestra]

Para cada pipeline_task que es cada una de las fuentes iniciales se corre su script de scrapping que descarga de forma local, después almacena en s3 guardando metadata del momento en que se ingestó, posteriormente se pasa al preprocesamiento que limpia y une las tablas con los históricos y se almacena en una tabla de postgres para el siguiente proceso de ETL.

En esta primera etapa lo único que hay que modificar para agregar una nueva fuente es un código de scrapping (ya sea en bash o python) que se almacena en la carpeta de ingesta y se modifica common/pg_raw_schemas para hacer la incerción a la base de datos.  

La siguiente etapa es el pipeline de ETL que aplica reglas de limpieza y homologación para poder hacer los joins.

Dado que las bases de datos de compranet, declaranet, registro de funcionarios y unidades compradoras no tienen foreign keys para hacer los joins se emplearon métodos de string fuzzy matching implementados en postgres para unir los textos más similares.

Una vez que se realiza el merge entre las diferentes tablas y se logra crear el historial de los funcionarios públicos se hace la ingesta dentro de la base de datos neo4j también inicializada en el ambiente de docker-compose.

La última etapa del pipeline corre las medidas de centralidad ..