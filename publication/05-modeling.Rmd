# Feature selection and Modeling

It is nearly impossible to correctly label all observations or agents as corrupt. We decided then to characterize possibly corrupt cases by the suspicious behaviour of agents involved in the process, so we looked at two main dimensions:

## Missing Data

Owing to the ammount of missing and dubious information found on Compranet's database, although not necessarily related to corruption, we created an index that grades agencies and RUs according to the level of clarity in which they report public infromation. The index is relative to both the ammount of missing or dubious observations for each variable and for each agency or RU.

Specifically, we counted the missing observations in each column by agency and by RU and weighted such count, first, by the percentage of missing observations for each agency or RU relative to the sum of missing observations in the column, and second, by the percentage of observations each agency or RU has within the column. Finally, we integrated the results for all columns into a single index by agency or RU by adding them.

$$ I_i = \sum_j \text{missing}_{i,j} * \frac{\text{missing}_j}{\text{missing}_{i,j}} * \frac{\text{observations}_j}{\text{observations}_{i,j}} $$ 
where $j$ represnets columns, and $i$ represents either agencies or RUs.

## Compliance with Transparency Laws

Although the way in which RUs report some of the information may not be ideal for its analysis, in does not mean that they are not complying with transparency laws regarding government procurements.

To have an accurate indicator on compliance with transparency laws and evaluate such compliance in view of corruption in government procurements, we studied carefully the functioning of Compranet's platform to identify deliberate omissions of information. These deliberate omissions were coded in a different way than missing values corresponding to procurement aspects which specific procurements don't have access to; this was done in a careful manner, cross-referncing with other existing databases, such as RUPC (ahort for 'Registro Unico de Proveedores y Contratistas), before asigning codes to missing observations.

The new codification of missing values allows to apply the missing data index described above in order to grade dependencies and buying units according to their level of compliance with transparency laws regarding government procurements.

# Graph Analysis

## Suspicious Connections 

Our data modeling process implies creating a fairly complex multi layered graph, for which not all centrality measures can be computed. We combined graph structure measures with more intuitive variables to include in our analysis: 

### Weighted degree centrality of bureaucrat $i$, firm $j$ or RU $u$

Weighted degree centrality of some vertex $v$ is defined as follows: 

$$C_{D}(v) := deg(v)$$

### Concentration of bureaucrat connections

Inspired in the construction of Hirschman Herfindahl Index (HHI), this variable is constructed as: 

$$H_i = \sum_j w_{i,j}^2$$

where $w_{i,j}$ is weight of firm $j$ in the total value assigned through contracts by bureaucrat $i$. 

### Concentration of firm connections

This is intuitively seen as some probability that firm $j$ has at least one crony relationship. 

$$\bar{w_k} = \underset{i}{\text{max  } w_{i,k}}$$

### Distance to other agents

We also computed the shortest paths distance to a sanctioned firm, to a ghost firm or to a bureaucrat using Dijkstra's algorithm. The shortest path distance calculates the number of nodes that must be visited when connecting two specific nodes, adding a unit to the distance for each node visited. It does this for all possible paths between two nodes and then selects the minimum distance.

In general, Dijkstra's algorithm assigns some initial distance values and will try to improve them step by step as explained below:

1. Assign to every node an initial distance value.
2. Sets the initial node as the current node and marks all other nodes unvisited. 
3. Creates a set of all the unvisited nodes, called the unvisited set.
4. For the current node, it considers all of its neighbors and calculates their tentative distances. It then compares the newly calculated tentative distance to the current assigned value and assigns the smaller one. Otherwise, it keeps the current value.
5. When  all of the neighbors of the current node have been considered, it marks the current node as visited and removes it from the unvisited set. A visited node will never be checked again.
6. The algorithm reaches an end when either the destination node is no longer in the unvisited set or when the distance among the unvisited nodes is infinity (i.e. there is no connection between the nodes).
7. If the algorithm does not reach an end, it selects the unvisited node that is marked with the smallest tentative distance, sets it as the new "current node", and goes back to step 4

In later steps we will use the shortest path to identify centrality meassures in the graph.